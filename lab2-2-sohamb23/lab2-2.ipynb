{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "     \n",
    "       Prints the result to stdout and returns the exit status. \n",
    "       Provides a printed warning on non-zero exit status unless `warn` \n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs187-2021/lab2-2.git .tmp\n",
    " mv .tmp/tests ./\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a614706",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# CS187\n",
    "## Lab 2-2 – Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've read about recurrent neural networks, but there's nothing like carrying out the calculations yourself to really understand what's going on in these systems. In this lab, you'll carry out RNN calculations by hand or by programming – both forward (calculating outputs from inputs) and backward (computing gradients) – and use RNNs to simulate bigram language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
    "\n",
    "* [`torch.clamp`](https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp): restricts all elements of a tensor to a specific range\n",
    "* [`torch.diag`](https://pytorch.org/docs/stable/generated/torch.diag.html#torch-diag): creates a tensor with the given inputs as the diagonal\n",
    "* [`torch.eye`](https://pytorch.org/docs/stable/generated/torch.eye.html#torch-eye): creates an identity matrix\n",
    "* [`torch.mv`](https://pytorch.org/docs/stable/generated/torch.mv.html#torch-mv) (typically invoked via the `@` operator): matrix-vector multiplication\n",
    "* [`torch.prod`](https://pytorch.org/docs/stable/generated/torch.prod.html#torch-prod): takes the product of elements in a vector\n",
    "* [`torch.T`](https://pytorch.org/docs/stable/generated/torch.t.html#torch.t): returns the transpose of a tensor\n",
    "* [`torch.zeros`](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch-zeros): creates a matrix of zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation – Loading packages {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                                ]    0 / 4321\r",
      "100% [................................................................................] 4321 / 4321"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import wget\n",
    "import math\n",
    "\n",
    "# Script for visualizing computation graphs\n",
    "data_dir = \"data/\"\n",
    "sys.path.append(data_dir)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "wget.download('https://raw.githubusercontent.com/nlp-course/data/master/scripts/makedot.py', out=data_dir)\n",
    "from makedot import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The forward step\n",
    "\n",
    "A recurrent neural network (RNN) works by calculating a sequence of hidden states $\\vect{h} = \\langle h_0, \\ldots, h_N \\rangle$ and outputs $\\vect{o} = \\langle o_1, \\ldots, o_N \\rangle$ from a sequence of inputs $\\vect{x} =  \\langle x_1, \\ldots, x_N \\rangle$. (We're notating the elements, like $x_t$, $h_t$, and $o_t$ as if they are scalars, but you should keep in mind that they might well be vectors themselves.)\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/rnn-unfolded-figure.png\" width=350 align=right />\n",
    "Each hidden state $h_t$ and output $o_t$ of an RNN is calculated from an input $x_{t}$ and the previous hidden state $h_{t - 1}$ using a set of weights $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$ according to the following equations. (We ignore all bias terms for simplicity, and due to the fact that in large neural networks they make no big difference.)\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= \\sigma(\\vect{U}x_t + \\vect{V}h_{t - 1}) && \\mbox{hidden state} \\\\\n",
    "o_t &= \\vect{W}h_t && \\mbox{output score} \\\\\n",
    "\\hat{y}_t &= \\sigma(o_t) && \\mbox{predicted output distribution}\n",
    "\\end{align}\n",
    "\n",
    "The figure at right may be helpful in giving a picture of the RNN computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_numbering\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question:** To check your understanding, notice that we've defined $\\vect{h}$ so that it has one more element than $\\vect{x}$ and $\\vect{o}$. Why is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66d763",
   "metadata": {},
   "source": [
    "h has one more element than x or o because it needs to store the previous element in the vector when it is computing the next time step from $h_0$, which carries over throughout the rest of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Defining the RNN and its input\n",
    "\n",
    "To better understand this process, we set up an example of some RNN parameters and values for $x_1$ and $x_2$ in the next cell, so that you can carry out the RNN operations yourself to calculate $h_1$, $h_2$, $o_1$, and $o_2$. (In this example, the length of the input $N$ is 2, and the dimensionality of the $x$, $h$, and $o$ vectors are also 2. Thus, $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$ are all $2\\times 2$ matrices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN parameters\n",
    "U = torch.Tensor([ [-0.3,  0.6],\n",
    "                   [ 0.2,  0.1] ])\n",
    "V = torch.Tensor([ [ 0.4,  0.4],\n",
    "                   [ 0.9, -0.7] ])\n",
    "W = torch.Tensor([ [ 0.2,  0.1],\n",
    "                   [-0.2,  0.5] ])\n",
    "\n",
    "# inputs\n",
    "x1 = torch.Tensor([0.0, 1.0])\n",
    "x2 = torch.Tensor([0.3, 0.4])\n",
    "\n",
    "# initial value for the hidden state, a zero vector\n",
    "h0 = torch.Tensor([0, 0])\n",
    "\n",
    "# Set which nodes to visualize for later\n",
    "visualized_nodes = [U, V, W, x1, x2, h0]\n",
    "for p in visualized_nodes:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Carrying out the forward step\n",
    "\n",
    "Given these definitions, calculate values for $h_1$, $h_2$, $o_1$, and $o_2$ using `torch` operations.\n",
    "\n",
    "You may assume for this problem that the nonlinearity $\\sigma$ used in calculating $h_t$ is a Rectified Linear Unit (ReLU), defined by \n",
    "\n",
    "$$\\operatorname{ReLU}(x) = \\max(0, x)$$ \n",
    "\n",
    "> ReLU can be implemented as `torch.clamp(x, min=0)`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: forward\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate h1, h2, o1, and o2 using torch operations\n",
    "h1 = torch.clamp(torch.mv(U,x1) + torch.mv(V,h0),min =0)\n",
    "o1 = torch.mv(W,h1)\n",
    "h2 = torch.clamp(torch.mv(U,x2) + torch.mv(V,h1),min =0)\n",
    "o2 = torch.mv(W,h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f562fbf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the results for manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1:\ttensor([ 0.1300, -0.0700], grad_fn=<MvBackward>) \n",
      "o2:\ttensor([0.1430, 0.1990], grad_fn=<MvBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (f\"o1:\\t{o1} \\n\"\n",
    "       f\"o2:\\t{o2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the computation graph\n",
    "\n",
    "Now, we can visualize the computation graph, that is, the graph of how $o_1$ and $o_2$ are computed from other variables and parameters. \n",
    "\n",
    "> Note that to make the below code work, you need to install `graphviz`:\n",
    ">\n",
    "> * MacOS: `brew install graphviz`\n",
    "> * Deepnote: already installed\n",
    "> * Google Colab: already installed\n",
    "> * Ubuntu: `sudo apt-get install graphviz`\n",
    ">\n",
    "> On Deepnote, the below code might not show anything, but you can check the generated PDF in `computation_graph.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/09/v1n7m90x4x19d0b5zn46swr80000gn/T/ipykernel_13623/538765363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save Graph to `computation_graph.pdf`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'computation_graph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Visualize (not working in Deepnote)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         rendered = backend.render(self._engine, format, filepath,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                   quiet=quiet)\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "# Define the mapping from variable name to variable,\n",
    "# so that the nodes in our computation graph can be labeled\n",
    "params = {k: eval(k) for k in ['U', 'V', 'W', \n",
    "                               'x1', 'x2', \n",
    "                               'h0', \n",
    "                               'h1', 'o1', \n",
    "                               'h2', 'o2']}\n",
    "\n",
    "# Visualize the computation graph constructed by PyTorch\n",
    "dot = make_dot((o1, o2), params=params)\n",
    "\n",
    "# Save Graph to `computation_graph.pdf`\n",
    "dot.render(data_dir + 'computation_graph')\n",
    "\n",
    "# Visualize (not working in Deepnote)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the generated computation graph what you expected? Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressing bigram language models via RNNs\n",
    "\n",
    "In this section, as an exercise in understanding how RNNs work, you'll design an RNN to behave like a bigram model. By providing a reduction (remember CS121?) from bigram models to RNNs, you thereby show that RNNs are (not surprisingly) more expressive than bigram language models.\n",
    "\n",
    "## Bigram language models\n",
    "\n",
    "Recall that a bigram language model uses the previous word to predict the next word in a sequence $w_1 \\cdots w_N$. A bigram model over a vocabulary $\\vect{v} = \\{v_1, \\ldots, v_V\\}$ is specified by a set of probabilities $\\Prob(w_{t+1} = v_j \\given w_t = v_i)$, the probability that a word of type $v_j$ follows a word of type $v_i$. \n",
    "(As usual, we'll abbreviate this probability $\\Prob(v_j \\given v_i)$, since the probability is assumed to be the same for all $t$.)\n",
    "\n",
    "We can pack these probabilities into a single table $T$ with $V$ rows and $V$ columns such that  \n",
    "\n",
    "$$T_{ij} = \\Prob(v_j \\given v_i)$$\n",
    "\n",
    "Importantly, the sum $\\sum_{j=1}^{V} T_{ij}$ is $1$ for all $i$. The vector $T_i$ (the $i$-th row of table $T$) thus constitutes the probability distribution for the word following $v_i$.\n",
    "\n",
    "For this activity, we use a vocabulary with only two tokens (we can think of them as $a$ and $b$) encoded as one-hot vectors ($v_1=$ `[1,0]` and $v_2=$  `[0,1]`) and we provide a transition table $T$. We also define two sample sequences representing $aaaaaabbaa$ and $babbbbabbb$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2799999994039535\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# the vocabulary V\n",
    "Vocab = torch.Tensor([ [1, 0], \n",
    "                       [0, 1] ])\n",
    "\n",
    "# the bigram probabilities T_ij\n",
    "T = torch.Tensor([ [0.6, 0.4],\n",
    "                   [0.3, 0.7] ])\n",
    "print(T[0][1].item() * T[1][1].item())\n",
    "\n",
    "# two sample sequences\n",
    "seq1 = torch.Tensor(\n",
    "    [ [1,0], [1,0], [1,0], [1,0], [1,0], \n",
    "      [1,0], [0,1], [0,1], [1,0], [1,0] ])\n",
    "print(seq1[1][1])\n",
    "seq2 = torch.Tensor(\n",
    "    [ [0,1], [1,0], [0,1], [0,1], [0,1],\n",
    "      [0,1], [1,0], [0,1], [0,1], [0,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Before proceeding, take a guess as to which of the two sequences would be more likely according to the provided bigram model. (We won't hold you to the guess.)\n",
    "\n",
    "Now write a function `sequence_probability` to find the probability of a sequence according to the bigram model. Below, we'll use that function to check your guess.\n",
    "\n",
    "> As in the previous lab, we'll ignore the contribution of the first $n-1$ tokens (the first token in the case of this example), since it doesn't have sufficient context. In a full $n$-gram model, we'd pad the start of the string with $n-1$ \"start of sequence\" tokens that are not part of the main vocabulary and have probability 1. It's standard also to add an \"end of sequence\" token at the end as well. But we'll pass on these niceties for now, if for no other reason than that it would double the size of the vocabulary you'd have to deal with.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: bigram\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# TODO -- Write a function to find the probability of a sequence given a bigram table\n",
    "def sequence_probability(seq, T):\n",
    "    \"\"\"Returns the probability of a sequence `seq` under a bigram table `T`.\n",
    "    Arguments:\n",
    "      seq: a sequence of size N x vocab_size, where seq[i] is the one-hot\n",
    "           representation for the i-th word.\n",
    "      T: a bigram table, where T_{ij} = P(v_j | v_i).\n",
    "    Returns: the probability of the sequence, a tensor of a single element.\"\"\"\n",
    "    prob = 1\n",
    "    for i in range(len(seq)-1):\n",
    "        rowIndex = int(seq[i][1].item())\n",
    "        columnIndex = int(seq[i+1][1].item())\n",
    "        prob *= T[rowIndex][columnIndex].item()\n",
    "    return prob\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "956cf0ad",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sequence_probability` function to find the probabilities of the two sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of A: 0.00392\n",
      "Probability of B: 0.00242\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability of A: {sequence_probability(seq1, T):.5f}\\n\"\n",
    "      f\"Probability of B: {sequence_probability(seq2, T):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was your guess correct? Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Tiny bigram RNN\n",
    "\n",
    "In theory, given enough capacity, [an RNN can approximate any function arbitrarily well](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.7590&rep=rep1&type=pdf). (In practice, we don't have infinite capacity, and even if we did have sufficient capacity, that doesn't mean that a particular optimization method, like stochastic gradient descent as you've been using in project segment 1, can find the global optimum.) \n",
    "\n",
    "In this section, you'll show that an RNN is at least as expressive as a bigram language model. You'll design the activation function $\\sigma$ and the parameters $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$ of an RNN such that it behaves exactly like the prticular bigram model above, by taking as input a sequence of one-hot representations of words, and outputting at each step a vector with the probabilities of the next word.\n",
    "\n",
    "For instance, the probabilities for the word following $v_i$ should be $T_i$, the row from the transition matrix. Given a sequence beginning with $v_1$, represented by the one-hot encoding [1, 0], your RNN's first output should be\n",
    "\n",
    "$$ o_1 = \\vect{W}h_1= \\vect{W} \\sigma\\left(\\vect{U} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \n",
    "                            + \\vect{V} h_0\n",
    "                            \\right) \n",
    "       = T_1 $$\n",
    "\n",
    "etc. Assume that $h_0$, the initial $h$ value, is $\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. \n",
    "\n",
    "> Hint 1: Use $\\sigma(x) = x$ for simplicity. \n",
    ">\n",
    "> Hint 2: You're going to want to work this out on paper before filling in your solution in the next cell.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: tiny_bigram_RNN\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO -- Define the parameters of the model in terms of the bigram probability matrix `T` and other constants\n",
    "h0 = torch.zeros(2)\n",
    "sigma = lambda x: x\n",
    "U = T.T\n",
    "V = torch.zeros(2)\n",
    "W = torch.eye(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4ee8509",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"tiny_bigram_RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Consensus section: General bigram RNN\n",
    "\n",
    "> **This section is more open-ended in nature and need only be turned in for the consensus submission of the lab.**\n",
    "\n",
    "Above, you worked out the solution for implementing a bigram model for a two-word vocabulary as an RNN. But the construction can be generalized for bigram models over vocabularies of arbitrary size, say, $N$ word types.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_bigram\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "---\n",
    "\n",
    "**Question:** \n",
    "In general, given an $N \\times N$ bigram probability matrix $T_{ij}$ what would be the activation function $\\sigma$ and the parameters $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$,  of an RNN that outputs $T_i$ given $v_i$ as an input?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bfec3",
   "metadata": {},
   "source": [
    "The activation function $\\sigma$ could remain the same, in order to maintain the fact that $T_i$ will be outputted when $v_i$ is an input. Similarly, $U$ could continue to be $T.T$ and $V$ could instead be a n-dimensional vector of zeros instead of a 2-dimensional vector of zeros and $W$ could be an $n*n$ identity matrix instead of a $2*2$ identity matrix. The whole problem could be scaled up to $n$ dimensions instead of 2 dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# The backward step\n",
    "\n",
    "The remainder of the lab needs you to do math derivations. In these exercises you will both build your intuition and gain a concrete understanding of how back-propagation works in an RNN.\n",
    "\n",
    "Back-propagation is the method used to compute the gradients of an RNN. First, the RNN runs forward on some inputs, and then we calculate the loss as a function of the output (a measure of how far off the RNN's output $o_t$ was from the desired output $y_t$). The loss function we will be using is the squared error:\n",
    "\n",
    "$$ L = \\sum_{t = 1}^{N} (y_{t} - o_{t})^2 $$\n",
    "\n",
    "For the particular case of a language model, the \"desired output\" is just the next word in the sequence, so we have \n",
    "\n",
    "$$ L = \\sum_{t = 1}^{N} (x_{t+1} - o_{t})^2 $$\n",
    "\n",
    "> There's a little issue with the last step, since $x_{N+1}$ doesn't exist. Let's pretend that $x_{N+1}$ exists below for brevity of notations. (See the comment above about end-of-sequence tokens.)\n",
    "\n",
    "> Note that we use squared error loss here mainly for simplicity. In real language modeling tasks people use cross entropy loss, which you've seen in project segment 1.\n",
    "\n",
    "We then find the derivatives of the loss with respect to each parameter and use the derivatives to make small adjustments to the parameters to reduce the loss. This process is repeated until the loss is minimized.\n",
    "\n",
    "To minimize the loss function, we need to calculate the derivative of the loss $L$ with respect to all parameters. In this lab, we only consider how to calculate the derivative of $L$ with respect to $\\vect{U}$, but other parameters work similarly.\n",
    "\n",
    "For simplicity, let's assume for now that $h_0, \\ldots, h_N$, $x_1, \\ldots, x_N$ and $o_1, \\ldots, o_N$ are all scalars. Therefore, the parameters $\\vect{U}$, $\\vect{V}$, $\\vect{W}$ are all scalars as well. Such an assumption avoids taking gradients of vectors and matrices, although the below results can be easily generalized.\n",
    "\n",
    "In the next few subsections, you'll derive the gradient formulas for RNNs operating on sequences first of length 1, then 2, then 3, and finally on arbitrary length sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN backprop on very very short sequences\n",
    "\n",
    "Consider an RNN run on an input sequence of length 1. This RNN's output sequence will therefore consist only of a single output $o_1$.\n",
    "\n",
    "As you can see from the loss function given above, $L = (x_2 - o_1)^2$ is a function of $o_1$ and $x_2$, therefore we can find \n",
    "\n",
    "$$\\frac{\\partial L(o_1, x_2)}{\\partial o_1} = 2(o_1 - x_2)$$\n",
    "\n",
    "Furthermore, $o_1$ is a function of $h_1$ and $\\vect{W}$, so we can find $\\frac{\\partial o_1(h_1, \\vect{W})}{\\partial h_1}$. Finally, $h_1$ is a function of $\\vect{U}$, $\\vect{V}$, $h_0$, and $x_1$, so we can find $\\frac{\\partial h_1(\\vect{U}, \\vect{V}, h_0, x_1)}{\\partial \\vect{U}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's visualize the computation graph for more intuition. Note that different from the previous computation graph, we only show the variables that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/09/v1n7m90x4x19d0b5zn46swr80000gn/T/ipykernel_13623/1805873050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Save Graph to `computation_graph_1.pdf`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'computation_graph_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         rendered = backend.render(self._engine, format, filepath,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                   quiet=quiet)\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "# RNN parameters\n",
    "U_ = torch.Tensor([0.1]) # we use U_ instead of U (etc.) to avoid autograding issues\n",
    "V_ = torch.Tensor([0.2])\n",
    "W_ = torch.Tensor([0.3])\n",
    "\n",
    "# inputs\n",
    "x1_ = torch.Tensor([0.5])\n",
    "x2_ = torch.Tensor([0.6])\n",
    "x3_ = torch.Tensor([0.7])\n",
    "\n",
    "# initial value for the hidden state, a zero vector\n",
    "h0_ = torch.Tensor([0])\n",
    "\n",
    "# Set which nodes to visualize later\n",
    "visualized_nodes = [U_, ]\n",
    "for p in visualized_nodes:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Calculate h1, o1\n",
    "h1_ = torch.clamp(U_ * x1_ + V_ * h0_, min=0)\n",
    "o1_ = W_ * h1_\n",
    "\n",
    "L_ = (o1_ - x2_) ** 2\n",
    "\n",
    "params = {k: eval(k+'_') for k in ['L', 'U', 'h1', 'o1']}\n",
    "dot = make_dot(L_, params=params)\n",
    "# Save Graph to `computation_graph_1.pdf`\n",
    "dot.render(data_dir + 'computation_graph_1')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** Find a formula for $\\frac{\\partial L(\\vect{U}, \\vect{V}, \\vect{W}, x_1, x_2, h_0)}{\\partial \\vect{U}}$ in terms of $\\frac{\\partial L(o_1, x_2)}{\\partial o_1}$, $\\frac{\\partial o_1(\\vect{W}, h_1)}{\\partial h_1}$ ,and $\\frac{\\partial h_1(\\vect{U}, \\vect{V}, h_0, x_1)}{\\partial \\vect{U}}$. You might find the above computation graph useful: how does $L$ depend on $\\vect{U}$? \n",
    "\n",
    ">Note that in your answer you can omit the arguments of the functions for brevity (e.g., you can use $\\frac{\\partial L}{\\partial o_1}$ instead of $\\frac{\\partial L(o_1, x_2)}{\\partial o_1}$), but keep in mind that the arguments are important: e.g., $o_1$ eventually depends on $\\vect{U}$, so $\\frac{\\partial o_1(\\vect{U}, \\vect{V}, \\vect{W}, x_1, x_2, h_0)}{\\partial \\vect{U}}$ is likely not zero, but if instead we assume its arguments are $h_1$ and $\\vect{W}$, i.e., $o_1 = o_1(h_1, \\vect{W})$, then $\\frac{\\partial o_1}{\\partial U}$ is undefined or 0, as when we take partial derivatives, we hold other arguments ($h_1$ and $\\vect{W}$ in this case) as constant.\n",
    "\n",
    ">Hint: Use the chain rule.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_backprop1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ebae4",
   "metadata": {},
   "source": [
    "$2(o_1 - x_2) * W * x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## RNN backprop on very short sequences\n",
    "\n",
    "Consider an RNN run on an input sequence of length 2. This RNN's output sequence $o$ will consist of $o_1$ and $o_2$ and the loss will be\n",
    "\n",
    "$$ L = (x_2 - o_1)^2 + (x_3 - o_2)^2 $$\n",
    "\n",
    "Let's visualize the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/09/v1n7m90x4x19d0b5zn46swr80000gn/T/ipykernel_13623/2292239404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Save Graph to `computation_graph_2.pdf`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'computation_graph_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         rendered = backend.render(self._engine, format, filepath,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                   quiet=quiet)\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "# RNN parameters\n",
    "U_ = torch.Tensor([0.1])\n",
    "V_ = torch.Tensor([0.2])\n",
    "W_ = torch.Tensor([0.3])\n",
    "\n",
    "# inputs\n",
    "x1_ = torch.Tensor([0.5])\n",
    "x2_ = torch.Tensor([0.6])\n",
    "x3_ = torch.Tensor([0.7])\n",
    "\n",
    "# initial value for the hidden state, a zero vector\n",
    "h0_ = torch.Tensor([0])\n",
    "\n",
    "# Set which nodes to visualize later\n",
    "visualized_nodes = [U_, ]\n",
    "for p in visualized_nodes:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Calculate h1, o1\n",
    "h1_ = torch.clamp(U_ * x1_ + V_ * h0_, min=0)\n",
    "h2_ = torch.clamp(U_ * x2_ + V_ * h1_, min=0)\n",
    "o2_ = W_ * h2_\n",
    "o1_ = W_ * h1_\n",
    "\n",
    "L_ = (o1_ - x2_)**2 + (o2_ - x3_)**2\n",
    "\n",
    "params = {k: eval(k+'_') for k in ['L', 'U', 'h1', 'o1', 'h2', 'o2']}\n",
    "\n",
    "dot = make_dot(L_, params=params)\n",
    "# Save Graph to `computation_graph_2.pdf`\n",
    "dot.render(data_dir + 'computation_graph_2')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** This time, find the formula for $\\frac{\\partial L(\\vect{U}, \\vect{V}, \\vect{W}, x_1, x_2, x_3, h_0)}{\\partial \\vect{U}}$, the derivative of the loss $L$ with respect to the parameter $\\vect{U}$ in terms of $\\frac{\\partial L(o_1, o_2, x_2, x_3)}{\\partial o_t}$, $\\frac{\\partial o_t( \\vect{W}, h_t)}{\\partial h_t}$, $\\frac{\\partial h_2(\\vect{U}, \\vect{V}, h_1, x_2)}{\\partial h_1}$, and $\\frac{\\partial h_t(\\vect{U}, \\vect{V}, h_{t-1}, x_t)}{\\partial \\vect{U}}$ for $t \\in \\{1,2\\}$. You might find the above computation graph useful.\n",
    "\n",
    "> How many possible paths are there in the computation graph from $\\vect{U}$ to $L$? Each path corresponds to a term in the final answer.\n",
    "\n",
    "> Hint: when we take $\\frac{\\partial h_2(\\vect{U}, \\vect{V}, h_1, x_2)}{\\partial \\vect{U}}$, we are holding other arguments (other than $\\vect{U}$) as constants, such as $h_1$. Therefore, $\\frac{\\partial h_2}{\\partial \\vect{U}}$ does not reflect the path through $h_1$ where $\\vect{U}$ can also exert influence on $h_2$: $\\vect{U} \\to h_1 \\to h_2$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_backprop2\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c99d1c",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## RNN backprop on short sequences\n",
    "\n",
    "For the penultimate backprop challenge, consider an RNN run on an input sequence of length 3. This RNN's output sequence $o$ will consist of $o_1$, $o_2$, and $o_3$.\n",
    "\n",
    "Let's look at the computation graph. How many possible paths are there from $\\vect{U}$ to $L$? How many $h$ elements are there in each path?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN parameters\n",
    "U_ = torch.Tensor([0.1])\n",
    "V_ = torch.Tensor([0.2])\n",
    "W_ = torch.Tensor([0.3])\n",
    "\n",
    "# inputs\n",
    "x1_ = torch.Tensor([0.5])\n",
    "x2_ = torch.Tensor([0.6])\n",
    "x3_ = torch.Tensor([0.7])\n",
    "x4_ = torch.Tensor([0.8])\n",
    "\n",
    "# initial value for the hidden state, a zero vector\n",
    "h0_ = torch.Tensor([0])\n",
    "\n",
    "# Set which nodes to visualize later\n",
    "visualized_nodes = [U_, ]\n",
    "for p in visualized_nodes:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Calculate h1, o1\n",
    "h1_ = torch.clamp(U_ * x1_ + V_ * h0_, min=0)\n",
    "h2_ = torch.clamp(U_ * x2_ + V_ * h1_, min=0)\n",
    "h3_ = torch.clamp(U_ * x3_ + V_ * h2_, min=0)\n",
    "o3_ = W_ * h3_\n",
    "o2_ = W_ * h2_\n",
    "o1_ = W_ * h1_\n",
    "\n",
    "L_ = (o1_ - x2_)**2 + (o2_ - x3_)**2 + (o3_ - x4_)**2\n",
    "\n",
    "params = {k: eval(k+'_') for k in ['L', 'U', 'h1', 'o1', 'h2', 'o2', 'h3', 'o3']}\n",
    "\n",
    "dot = make_dot(L_, params=params)\n",
    "# Save Graph to `computation_graph_3.pdf`\n",
    "dot.render(data_dir + 'computation_graph_3')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** This time, find the formula for $\\frac{\\partial L(\\vect{U}, \\vect{V}, \\vect{W}, x_1, x_2, x_3, x_4, h_0)}{\\partial \\vect{U}}$ in terms of $\\frac{\\partial L(o_1,o_2,o_3,x_2,x_3,x_{4})}{\\partial o_t}$, $\\frac{\\partial o_t(\\vect{W}, h_t)}{\\partial h_t}$, $\\frac{\\partial h_t(\\vect{U}, \\vect{V}, h_{t-1}, x_t)}{\\partial h_{t-1}}$, and $\\frac{\\partial h_t(\\vect{U}, \\vect{V}, h_{t-1}, x_t)}{\\partial \\vect{U}}$ for $t \\in \\{1, 2, 3\\}$.\n",
    "\n",
    "> You should start to see a pattern emerging from this solution.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_backprop3\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627bdef",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Looking back\n",
    "\n",
    "In the last few problems your answers were in terms of partial derivatives that we gave you. In this exercise you will calculate part of a partial derivative.\n",
    "\n",
    "**Question:** Referring back to the first problem in this lab, recall that \n",
    "\n",
    "$$ \\vect{U} = \\begin{bmatrix} -0.3 & 0.6 \\\\ 0.2 & 0.1 \\end{bmatrix}, \\vect{V} = \\begin{bmatrix} 0.4 & 0.4 \\\\ 0.9 & -0.7 \\end{bmatrix}, x_1 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix},   h_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$\n",
    "               \n",
    "Find the partial derivative $\\frac{\\partial h_{1}}{\\partial U_{1,1}}$, where $U_{1,1}$ is the element in the first row and first column of the matrix $\\vect{U}$.\n",
    "\n",
    "> **Hint:** We didn't tell you what the function $\\sigma$ is. Turns out you will not need it! If you set it up right, you will see that you will not have to do much math at all.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_looking_back\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68882cb",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now, let's verify your solution above using PyTorch, which can calculate gradients automatically. We show an example below of how to calculate gradients. Adapt it to find the gradients of $\\frac{\\partial h_{1,1}}{\\partial \\vect{U}}$ (where $h_{1,1}$ denotes the first element of vector $h_1$, and here we are taking the derivative with respect to the entire matrix $\\vect{U}$). Use ReLU as $\\sigma$.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: automatic_diff\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify the code below to find the gradients of \n",
    "# $\\frac{\\partial h_{1,1}}{\\partial \\vect{U}}$\n",
    "\n",
    "# RNN parameters\n",
    "U_ = torch.Tensor([ [-0.3,  0.6],\n",
    "               [ 0.2,  0.1] ])\n",
    "V_ = torch.Tensor([ [ 0.4,  0.4],\n",
    "               [ 0.9, -0.7] ])\n",
    "U_.requires_grad = True\n",
    "\n",
    "# inputs\n",
    "x1_ = torch.Tensor([0.0, 1.0])\n",
    "\n",
    "# initial value for the hidden state, a zero vector\n",
    "h0_ = torch.Tensor([0, 0])\n",
    "h1_ = ...\n",
    "h11 = ...\n",
    "\n",
    "# The magic gradient computation\n",
    "h11.backward()\n",
    "U_grad = U_.grad\n",
    "print (f'Gradient of W: {U_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a8ed0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"automatic_diff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the computed gradient agree with your calculation of $\\frac{\\partial h_{1}}{\\partial U_{1,1}}$? (Note that here you only calculated $\\frac{\\partial h_{1,1}}{\\partial \\vect{U}}$ through PyTorch, so let's just focus on $\\frac{\\partial h_{1,1}}{\\partial U_{1,1}}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, hopefully you have got some idea of how PyTorch computes gradients with respect to $\\vect{U}$ by calling a `backward` function on `h11`: the reason is that PyTorch maintains an underlying computation graph, and it can find all ancestors of `h11` (including $\\vect{U}$). The gradient computation process is essentially applying chain rules on this computation graph. (Think of how you would implement it yourself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Optional section: RNN backprop on arbitrary sequences\n",
    "\n",
    "> **This section is more challenging in nature and is therefore completely optional and will not affect your grade.**\n",
    "\n",
    "The final challenge! Consider an RNN run on an input sequence of arbitrary length $N$. \n",
    "\n",
    "**Question:** Find a general formula for $\\frac{\\partial L(\\vect{U}, \\vect{V}, \\vect{W}, x_1,\\cdots, x_{N+1}, h_0)}{\\partial \\vect{U}}$ in terms of $\\frac{\\partial L(o_1,\\cdots,o_N, x_2, \\cdots, x_{N+1})}{\\partial o_t}$, $\\frac{\\partial o_t(\\vect{W}, h_t)}{\\partial h_t}$, $\\frac{\\partial h_t(\\vect{U}, \\vect{V}, h_{t-1}, x_t)}{\\partial h_{t-1}}$, and $\\frac{\\partial h_t(\\vect{U}, \\vect{V}, h_{t-1}, x_t)}{\\partial \\vect{U}}$ for $t \\in \\{1, \\ldots, N\\}$.\n",
    "\n",
    "> Hint: How many terms are there when $N$ is 1, 2, or 3? How many possible paths are there from $\\vect{U}$ to $L$ in the computation graph?\n",
    "\n",
    "> Hint: Your solution might look a little like\n",
    "$$ \\sum + \\sum\\left(\\sum\\left(\\prod\\right)\\right) $$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_backprop4\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f559d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Lab debrief – for consensus submission only\n",
    "\n",
    "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
    "\n",
    "* Was the lab too long or too short?\n",
    "* Were the readings appropriate for the lab? \n",
    "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
    "* Are there additions or changes you think would make the lab better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9eb96",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# End of lab 2-2 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e60aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020fd06",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "CS187 Lab 2-2 – Recurrent neural networks"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
